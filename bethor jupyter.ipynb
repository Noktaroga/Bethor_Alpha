{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de URLs y sus ligas correspondientes\n",
    "urls = [\n",
    "    {\n",
    "        \"match_url\": \"https://fbref.com/en/comps/9/schedule/Premier-League-Scores-and-Fixtures\",\n",
    "        \"stats_url\": \"https://fbref.com/en/comps/9/Premier-League-Stats\",\n",
    "        \"liga\": \"Premier League\",\n",
    "        \"id\": \"sched_2023-2024_9_1\",\n",
    "    },\n",
    "    {\n",
    "        \"match_url\": \"https://fbref.com/en/comps/12/schedule/La-Liga-Scores-and-Fixtures\",\n",
    "        \"stats_url\": \"https://fbref.com/en/comps/12/La-Liga-Stats\",\n",
    "        \"liga\": \"La Liga\",\n",
    "        \"id\": \"sched_2023-2024_12_1\"\n",
    "    },\n",
    "    {\n",
    "        \"match_url\": \"https://fbref.com/en/comps/11/schedule/Serie-A-Scores-and-Fixtures\",\n",
    "        \"stats_url\": \"https://fbref.com/en/comps/11/Serie-A-Stats\",\n",
    "        \"liga\": \"Serie A\",\n",
    "        \"id\": \"sched_2023-2024_11_1\"\n",
    "    },\n",
    "    {\n",
    "        \"match_url\": \"https://fbref.com/en/comps/20/schedule/Bundesliga-Scores-and-Fixtures\",\n",
    "        \"stats_url\": \"https://fbref.com/en/comps/20/Bundesliga-Stats\",\n",
    "        \"liga\": \"Bundesliga\",\n",
    "        \"id\": \"sched_2023-2024_20_1\"\n",
    "    },\n",
    "    {\n",
    "        \"match_url\": \"https://fbref.com/en/comps/13/schedule/Ligue-1-Scores-and-Fixtures\",\n",
    "        \"stats_url\": \"https://fbref.com/en/comps/13/Ligue-1-Stats\",\n",
    "        \"liga\": \"Ligue 1\",\n",
    "        \"id\": \"sched_2023-2024_13_1\"\n",
    "    },\n",
    "    {\n",
    "        \"match_url\": \"https://fbref.com/en/comps/22/schedule/Major-League-Soccer-Scores-and-Fixtures\",\n",
    "        \"stats_url\": \"https://fbref.com/en/comps/22/Major-League-Soccer-Stats\",\n",
    "        \"liga\": \"MLS\",\n",
    "        \"id\": \"sched_2023-2024_22_1\"\n",
    "    },\n",
    "    {\n",
    "        \"match_url\": \"https://fbref.com/en/comps/30/schedule/Brasileiro-Serie-A-Scores-and-Fixtures\",\n",
    "        \"stats_url\": \"https://fbref.com/en/comps/30/Brasileiro-Serie-A-Stats\",\n",
    "        \"liga\": \"Brasileirão\",\n",
    "        \"id\": \"sched_2023-2024_30_1\"\n",
    "    },\n",
    "    {\n",
    "        \"match_url\": \"https://fbref.com/en/comps/23/schedule/Eredivisie-Scores-and-Fixtures\",\n",
    "        \"stats_url\": \"https://fbref.com/en/comps/23/Eredivisie-Stats\",\n",
    "        \"liga\": \"Eredivisie\",\n",
    "        \"id\": \"sched_2023-2024_23_1\"\n",
    "    },\n",
    "    {\n",
    "        \"match_url\": \"https://fbref.com/en/comps/32/schedule/Primeira-Liga-Scores-and-Fixtures\",\n",
    "        \"stats_url\": \"https://fbref.com/en/comps/32/Primeira-Liga-Stats\",\n",
    "        \"liga\": \"Primeira Liga\",\n",
    "        \"id\": \"sched_2023-2024_32_1\"\n",
    "    },\n",
    "    {\n",
    "        \"match_url\": \"https://fbref.com/en/comps/31/schedule/Liga-MX-Scores-and-Fixtures\",\n",
    "        \"stats_url\": \"https://fbref.com/en/comps/31/Liga-MX-Stats\",\n",
    "        \"liga\": \"Liga MX\",\n",
    "        \"id\": \"sched_2023-2024_31_1\"\n",
    "    },\n",
    "    {\n",
    "        \"match_url\": \"https://fbref.com/en/comps/34/schedule/Argentinos-Jrs-Scores-and-Fixtures\",\n",
    "        \"stats_url\": \"https://fbref.com/en/comps/34/Argentinos-Jrs-Stats\",\n",
    "        \"liga\": \"Liga Profesional Argentina\",\n",
    "        \"id\": \"sched_2023-2024_34_1\"\n",
    "    },\n",
    "    {\n",
    "        \"match_url\": \"https://fbref.com/en/comps/37/schedule/Belgian-Pro-League-Scores-and-Fixtures\",\n",
    "        \"stats_url\": \"https://fbref.com/en/comps/37/Belgian-Pro-League-Stats\",\n",
    "        \"liga\": \"Belgian Pro League\",\n",
    "        \"id\": \"sched_2023-2024_37_1\"\n",
    "    },\n",
    "    {\n",
    "        \"match_url\": \"https://fbref.com/en/comps/21/schedule/Primera-Division-Scores-and-Fixtures\",\n",
    "        \"stats_url\": \"https://fbref.com/en/comps/21/Primera-Division-Stats\",\n",
    "        \"liga\": \"Primera Division Argentina\",\n",
    "        \"id\": \"sched_2023-2024_21_1\"\n",
    "    },\n",
    "    {\n",
    "        \"match_url\": \"https://fbref.com/en/comps/189/schedule/Womens-Super-League-Scores-and-Fixtures\",\n",
    "        \"stats_url\": \"https://fbref.com/en/comps/189/Womens-Super-League-Stats\",\n",
    "        \"liga\": \"Womens Super League\",\n",
    "        \"id\": \"sched_2023-2024_189_1\"\n",
    "    },\n",
    "    {\n",
    "        \"match_url\": \"https://fbref.com/en/comps/230/schedule/Liga-F-Scores-and-Fixtures\",\n",
    "        \"stats_url\": \"https://fbref.com/en/comps/230/Liga-F-Stats\",\n",
    "        \"liga\": \"La Liga Femenil\",\n",
    "        \"id\": \"sched_2023-2024_230_1\"\n",
    "    },\n",
    "    {\n",
    "        \"match_url\": \"https://fbref.com/en/comps/182/schedule/NWSL-Scores-and-Fixtures\",\n",
    "        \"stats_url\": \"https://fbref.com/en/comps/182/NWSL-Stats\",\n",
    "        \"liga\": \"NWSL\",\n",
    "        \"id\": \"sched_2023-2024_182_1\"\n",
    "    },\n",
    "    {\n",
    "        \"match_url\": \"https://fbref.com/en/comps/196/schedule/A-League-Women-Scores-and-Fixtures\",\n",
    "        \"stats_url\": \"https://fbref.com/en/comps/196/A-League\",\n",
    "        \"liga\": \"A League\",\n",
    "        \"id\": \"sched_2023-2024_196_1\",\n",
    "    },\n",
    "    {\n",
    "        \"match_url\": \"https://fbref.com/en/comps/193/schedule/Division-1-Feminine-Scores-and-Fixtures\",\n",
    "        \"stats_url\": \"https://fbref.com/en/comps/193/Division-1-Feminine-Stats\",\n",
    "        \"liga\": \"Division 1 Feminine\",\n",
    "        \"id\": \"sched_2023-2024_193_1\"\n",
    "    },\n",
    "    {\n",
    "        \"match_url\": \"https://fbref.com/en/comps/183/schedule/Frauen-Bundesliga-Scores-and-Fixtures\",\n",
    "        \"stats_url\": \"https://fbref.com/en/comps/183/Frauen-Bundesliga-Stats\",\n",
    "        \"liga\": \"Frauen Bundesliga\",\n",
    "        \"id\": \"sched_2023-2024_183_1\"\n",
    "    },\n",
    "    {\n",
    "        \"match_url\": \"https://fbref.com/en/comps/208/schedule/Serie-A-Scores-and-Fixtures\",\n",
    "        \"stats_url\": \"https://fbref.com/en/comps/208/Serie-A-Stats\",\n",
    "        \"liga\": \"Serie A\",\n",
    "        \"id\": \"sched_2023-2024_208_1\"\n",
    "    },\n",
    "    {\n",
    "        \"match_url\": \"https://fbref.com/en/comps/26/schedule/Super-Lig-Scores-and-Fixtures\",\n",
    "        \"stats_url\": \"https://fbref.com/en/comps/26/Super-Lig-Stats\",\n",
    "        \"liga\": \"Super Lig\",\n",
    "        \"id\": \"sched_2023-2024_26_1\"\n",
    "    },\n",
    "    {\n",
    "        \"match_url\": \"https://fbref.com/en/comps/70/schedule/Saudi-Professional-League-Scores-and-Fixtures\",\n",
    "        \"stats_url\": \"https://fbref.com/en/comps/70/Saudi-Professional-League-Stats\",\n",
    "        \"liga\": \"Saudi Professional League\",\n",
    "        \"id\": \"sched_2023-2024_70_1\"\n",
    "    },\n",
    "    {\n",
    "        \"match_url\": \"https://fbref.com/en/comps/25/schedule/J1-League-Scores-and-Fixtures\",\n",
    "        \"stats_url\": \"https://fbref.com/en/comps/25/J1-League-Stats\",\n",
    "        \"liga\": \"J1 League\",\n",
    "        \"id\": \"sched_2023-2024_25_1\",   \n",
    "    },\n",
    "    {\n",
    "        \"match_url\": \"https://fbref.com/en/comps/340/schedule/Kvindeligaen-Scores-and-Fixtures\",\n",
    "        \"stats_url\": \"https://fbref.com/en/comps/340/Kvindeligaen-Stats\",\n",
    "        \"liga\": \"Kvindeligaen\",\n",
    "        \"id\": \"sched_2023-2024_340_1\"\n",
    "    },\n",
    "    {\n",
    "        \"match_url\": \"https://fbref.com/en/comps/195/schedule/Eredivisie-Vrouwen-Scores-and-Fixtures\",\n",
    "        \"stats_url\": \"https://fbref.com/en/comps/195/Eredivisie-Vrouwen-Stats\",\n",
    "        \"liga\": \"Eredivisie Vrouwen\",\n",
    "        \"id\": \"sched_2023-2024_195_1\"\n",
    "    },\n",
    "    {\n",
    "        \"match_url\": \"https://fbref.com/en/comps/187/schedule/Damallsvenskan-Scores-and-Fixtures\",\n",
    "        \"stats_url\": \"https://fbref.com/en/comps/187/Damallsvenskan-Stats\",\n",
    "        \"liga\": \"Damallsvenskan\",\n",
    "        \"id\": \"sched_2023-2024_187_1\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "import random\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ESTE APARTADO ES SUPER IMPORTANTE Y DEBE SER DEJADO EN UN ARCHIVO APARTE, ESTE ES EL PUNTO DE ORIGEN PARA PODER HACER SCRAPPER, ESTO DEBE SER COMO TAL UNA CLASE O UNA FUNCION Y TODO EL CODIGO DEBE IR DISEÑADO A PARTIR DE ESTE NUCLEO DEBIDO A QUE SI NO, LAS CONSULTAS COMENZARAN A BLOQUEARSE Y SE ME HARA DIFICIL HACER EL SCRAPPING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directrices para un scraping responsable y menos detectable:\n",
    "# 0. Utiliza Proxy publicas de sitio de proxys publica, scrapear este sitio tambien.\n",
    "# 1. Utiliza `time.sleep` para introducir retrasos aleatorios entre las solicitudes y simular el comportamiento humano.\n",
    "# 2. Usa `requests.Session` para mantener el estado entre las solicitudes, lo que incluye cookies y cabeceras.\n",
    "# 3. Cambia el `User-Agent` y otros encabezados regularmente para simular diferentes navegadores y dispositivos.\n",
    "# 4. Considera rotar las direcciones IP utilizando proxies si realizas muchas solicitudes.\n",
    "# 5. Revisa y respeta el archivo `robots.txt` del sitio web para asegurarte de que tus acciones no violan las políticas del sitio.\n",
    "# 6. Sé consciente de la carga que tus acciones pueden imponer en los servidores del sitio web y ajusta la frecuencia de las solicitudes para minimizar el impacto.\n",
    "\n",
    "\n",
    "class SessionManager:\n",
    "    def __init__(self):\n",
    "        self.session = self.initialize_session()\n",
    "        self.ua = UserAgent()\n",
    "        self.update_headers()\n",
    "\n",
    "    def initialize_session(self):\n",
    "        \"\"\"\n",
    "        Inicializa y retorna una sesión de requests.\n",
    "        \"\"\"\n",
    "        return requests.Session()\n",
    "\n",
    "    def update_headers(self, referer=None):\n",
    "        \"\"\"\n",
    "        Actualiza los encabezados de la sesión, incluido el User-Agent y el Referer.\n",
    "        \"\"\"\n",
    "        referers = [\n",
    "            'https://www.google.com/', 'https://www.bing.com/',\n",
    "            'https://duckduckgo.com/', 'https://www.yahoo.com/',\n",
    "            'https://www.facebook.com/', 'https://twitter.com/'\n",
    "        ]\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': self.ua.random,\n",
    "            'Referer': referer if referer else random.choice(referers)\n",
    "        })\n",
    "\n",
    "    def make_request(self, url):\n",
    "        \"\"\"\n",
    "        Realiza una solicitud HTTP GET a la URL especificada y retorna el objeto Response.\n",
    "        \"\"\"\n",
    "        response = self.session.get(url)\n",
    "        time.sleep(random.uniform(1, 5))  # Simular comportamiento humano\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraccion de informacion referente a ligas, cuerpo de la consulta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FootballScraper(SessionManager):\n",
    "    \n",
    "    def scrape_league_data(self, url, liga):\n",
    "        \"\"\"\n",
    "        Scrapea datos de una liga específica desde la URL proporcionada, incluyendo la 'liga' como parámetro.\n",
    "        \"\"\"\n",
    "        self.update_headers(referer=url)  # Actualizar referer a la URL actual\n",
    "        response = self.make_request(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        table_id = self.get_table_id(soup)\n",
    "        if table_id:\n",
    "            return self.parse_league_table(soup, table_id, liga)\n",
    "        else:\n",
    "            print(f\"No se encontró la tabla de partidos en la página: {url}\")\n",
    "            return []\n",
    "\n",
    "    def get_table_id(self, soup):\n",
    "        \"\"\"\n",
    "        Obtiene dinámicamente el ID de la tabla de la liga desde el objeto BeautifulSoup dado.\n",
    "        \"\"\"\n",
    "        # Aquí implementa la lógica para obtener dinámicamente el ID de la tabla de la liga\n",
    "        # Puedes usar métodos de BeautifulSoup para encontrar la tabla de las ligas y extraer su ID\n",
    "        # Por ejemplo, podrías buscar ciertos elementos distintivos dentro de la página que contienen el ID de la tabla\n",
    "        # Retorna el ID de la tabla si se encuentra, o None si no se puede encontrar\n",
    "        # Ejemplo de implementación:\n",
    "\n",
    "        table = soup.find('table')\n",
    "        if table:\n",
    "            return table.get('id')\n",
    "        else:\n",
    "            print(\"No se encontró ninguna tabla en la página.\")\n",
    "            return None\n",
    "\n",
    "    def parse_league_table(self, soup, table_id, liga):\n",
    "        \"\"\"\n",
    "        Extrae y retorna datos de la tabla de la liga a partir del objeto BeautifulSoup dado, incluyendo la 'liga' como parámetro.\n",
    "        \"\"\"\n",
    "        table = soup.find('table', {'id': table_id})\n",
    "        matches = []\n",
    "        if table:\n",
    "            headers = [header.text.strip() for header in table.find('thead').find_all('th')]\n",
    "            for row in table.find('tbody').find_all('tr'):\n",
    "                cells = row.find_all('td')\n",
    "                if cells:\n",
    "                    match_info = {headers[i]: cells[i].text.strip() for i in range(len(cells))}\n",
    "                    match_info['Liga'] = liga\n",
    "                    matches.append(match_info)\n",
    "        else:\n",
    "            print(f\"No se encontró la tabla de partidos en la página con el ID: {table_id}\")\n",
    "        return matches\n",
    "\n",
    "\n",
    "    def extract_match_info(self, cells, liga):\n",
    "        \"\"\"\n",
    "        Extrae la información de un partido a partir de las celdas de una fila de la tabla.\n",
    "        \"\"\"\n",
    "        # Suponiendo que el 'match_report' se encuentra en la celda con índice 12 (el 13º elemento)\n",
    "        match_report = cells[12].text.strip() if len(cells) > 12 else \"No Report\"\n",
    "\n",
    "        match_info = {\n",
    "            \"Liga\": liga,\n",
    "            \"Week\": cells[0].text.strip(),\n",
    "            \"Day\": cells[1].text.strip(),\n",
    "            \"Date\": cells[2].text.strip(),\n",
    "            \"Time\": cells[3].text.strip(),\n",
    "            \"Home Team\": cells[4].text.strip(),\n",
    "            \"Home xG\": cells[5].text.strip(),\n",
    "            \"Score\": cells[6].text.strip(),\n",
    "            \"Away xG\": cells[7].text.strip(),\n",
    "            \"Away Team\": cells[8].text.strip(),\n",
    "            \"Attendance\": cells[9].text.strip(),\n",
    "            \"Venue\": cells[10].text.strip(),\n",
    "            \"Referee\": cells[11].text.strip(),\n",
    "            \"Match Report\": match_report,  # Asegúrate de que esta celda exista\n",
    "        }\n",
    "        return match_info\n",
    "\n",
    "    def scrape_team_stats(self, league_url, liga, table_id):\n",
    "        \"\"\"\n",
    "        Scrapea las estadísticas del equipo desde la página de estadísticas de la liga proporcionada.\n",
    "        Incluye 'liga' como parámetro para identificar la liga de la cual se están scrapeando las estadísticas.\n",
    "        'table_id' es el ID de la tabla que contiene las estadísticas en la página.\n",
    "        \"\"\"\n",
    "        self.update_headers(referer=league_url)\n",
    "        response = self.make_request(league_url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Intenta identificar la tabla por el ID proporcionado\n",
    "        table = soup.find('table', {'id': table_id})\n",
    "\n",
    "        # Si no encuentra la tabla con el ID proporcionado, usa una expresión regular para intentar identificarla\n",
    "        if not table:\n",
    "            regex_pattern = re.compile(r'results\\d{4}-\\d{4}\\d*_overall')\n",
    "            table = soup.find('table', id=regex_pattern)\n",
    "            if not table:\n",
    "                print(f\"No se encontró la tabla de estadísticas del equipo en la página: {league_url}\")\n",
    "                return []\n",
    "\n",
    "        teams_data = []\n",
    "        rows = table.find('tbody').find_all('tr')\n",
    "        for row in rows:\n",
    "            cells = row.find_all('td')\n",
    "            if cells:  # Si hay celdas en la fila, procesa la información\n",
    "                team_data = {\n",
    "                    'Liga': liga,\n",
    "                    'Team': cells[0].find('a').text.strip() if cells[0].find('a') else cells[0].text.strip(),\n",
    "                    'MP': cells[1].text.strip(),\n",
    "                    'W': cells[2].text.strip(),\n",
    "                    'D': cells[3].text.strip(),\n",
    "                    'L': cells[4].text.strip(),\n",
    "                    'GF': cells[5].text.strip(),\n",
    "                    'GA': cells[6].text.strip(),\n",
    "                    'GD': cells[7].text.strip(),\n",
    "                    'Pts': cells[8].text.strip(),\n",
    "                    'Pts/MP': cells[9].text.strip(),\n",
    "                    'xG': cells[10].text.strip(),\n",
    "                    'xGA': cells[11].text.strip(),\n",
    "                    'xGD': cells[12].text.strip(),\n",
    "                    'xGD/90': cells[13].text.strip(),\n",
    "                    'Attendance': cells[14].text.strip(),\n",
    "                    'Top Team Scorer': cells[15].text.strip(),\n",
    "                    'Goalkeeper': cells[16].text.strip(),\n",
    "                }\n",
    "                # Completa el resto de los campos team_data según la estructura de tu tabla\n",
    "                teams_data.append(team_data)\n",
    "            else:\n",
    "                print(f\"Fila incompleta detectada en la liga {liga}.\")\n",
    "\n",
    "        return teams_data\n",
    "    \n",
    "    def scrape_team_stats(self, league_url, liga, table_id):\n",
    "        \"\"\"\n",
    "        Scrapea las estadísticas del equipo desde la página de estadísticas de la liga proporcionada.\n",
    "        \"\"\"\n",
    "        self.update_headers(referer=league_url)\n",
    "        response = self.make_request(league_url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        table = soup.find('table', {'id': table_id})\n",
    "        if not table:\n",
    "            regex_pattern = re.compile(r'results\\d{4}-\\d{4}\\d*_overall')\n",
    "            table = soup.find('table', id=regex_pattern)\n",
    "            if not table:\n",
    "                print(f\"No se encontró la tabla de estadísticas del equipo en la página: {league_url}\")\n",
    "                return []\n",
    "\n",
    "        teams_data = []\n",
    "        rows = table.find('tbody').find_all('tr')\n",
    "        for row in rows:\n",
    "            cells = row.find_all('td')\n",
    "            # Asegúrate de que cada fila tenga el número mínimo esperado de celdas antes de procesar\n",
    "            if len(cells) >= 15:  # Ajusta este número según el mínimo de celdas esperadas en la estructura más corta\n",
    "                team_data = {\n",
    "                    'Rk': cells[0].text.strip() if len(cells) > 0 else 'N/A',\n",
    "                    'Squad': cells[1].text.strip() if len(cells) > 1 else 'N/A',\n",
    "                    'MP': cells[2].text.strip() if len(cells) > 2 else 'N/A',\n",
    "                    'W': cells[3].text.strip() if len(cells) > 3 else 'N/A',\n",
    "                    'D': cells[4].text.strip() if len(cells) > 4 else 'N/A',\n",
    "                    'L': cells[5].text.strip() if len(cells) > 5 else 'N/A',\n",
    "                    'GF': cells[6].text.strip() if len(cells) > 6 else 'N/A',\n",
    "                    'GA': cells[7].text.strip() if len(cells) > 7 else 'N/A',\n",
    "                    'GD': cells[8].text.strip() if len(cells) > 8 else 'N/A',\n",
    "                    'Pts': cells[9].text.strip() if len(cells) > 9 else 'N/A',\n",
    "                    'Pts/MP': cells[10].text.strip() if len(cells) > 10 else 'N/A',\n",
    "                    'Last 5': cells[11].text.strip() if len(cells) > 11 else 'N/A',\n",
    "                    'Attendance': cells[12].text.strip() if len(cells) > 12 else 'N/A',\n",
    "                    # Estos campos se inicializan aquí pero se pueden sobrescribir más adelante si están disponibles\n",
    "                    'Top Team Scorer': 'N/A',\n",
    "                    'Goalkeeper': 'N/A'\n",
    "                }\n",
    "\n",
    "                # Solo intenta acceder a campos adicionales si existen suficientes celdas\n",
    "            if len(cells) >= 19:  # Asegúrate de que el número aquí corresponda al total de campos esperados\n",
    "                team_data.update({\n",
    "                    'xG': cells[13].text.strip() if len(cells) > 13 else 'N/A',\n",
    "                    'xGA': cells[14].text.strip() if len(cells) > 14 else 'N/A',\n",
    "                    'xGD': cells[15].text.strip() if len(cells) > 15 else 'N/A',\n",
    "                    'xGD/90': cells[16].text.strip() if len(cells) > 16 else 'N/A',\n",
    "                    'Top Team Scorer': cells[17].find('a').text.strip() if cells[17].find('a') else cells[17].text.strip(),\n",
    "                    'Goalkeeper': cells[18].find('a').text.strip() if cells[18].find('a') else cells[18].text.strip()\n",
    "                })\n",
    "\n",
    "                teams_data.append(team_data)\n",
    "            else:\n",
    "                print(f\"Fila incompleta detectada en la liga {liga}. Se encontraron solo {len(cells)} celdas.\")\n",
    "\n",
    "        return teams_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'equipo'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m matches \u001b[38;5;241m=\u001b[39m scraper\u001b[38;5;241m.\u001b[39mscrape_league_data(match_url, liga)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m match \u001b[38;5;129;01min\u001b[39;00m matches:\n\u001b[1;32m---> 20\u001b[0m     guardar_datos_equipo([match], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtabla-partidos\u001b[39m\u001b[38;5;124m\"\u001b[39m, liga, \u001b[43mmatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mequipo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)  \u001b[38;5;66;03m# Asumiendo que 'match' contiene una clave 'equipo'\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Scrapear y procesar las estadísticas de los equipos\u001b[39;00m\n\u001b[0;32m     23\u001b[0m team_stats \u001b[38;5;241m=\u001b[39m scraper\u001b[38;5;241m.\u001b[39mscrape_team_stats(stats_url, liga, league\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid_stats\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mKeyError\u001b[0m: 'equipo'"
     ]
    }
   ],
   "source": [
    "# Instanciar el scraper\n",
    "scraper = FootballScraper()\n",
    "\n",
    "# Instanciar el procesador de datos\n",
    "processor = DataProcessor()\n",
    "\n",
    "# Iterar sobre las URLs y realizar el scraping para cada una\n",
    "for league in urls:\n",
    "    try:\n",
    "        match_url = league[\"match_url\"]  # URL para los partidos de la liga\n",
    "        stats_url = league[\"stats_url\"]  # URL para las estadísticas de los equipos de la liga\n",
    "        liga = league[\"liga\"]  # Nombre de la liga\n",
    "\n",
    "        # Obtener el ID de la tabla de partidos\n",
    "        match_response = scraper.make_request(match_url)\n",
    "        match_soup = BeautifulSoup(match_response.text, 'html.parser')\n",
    "        table_id_matches = scraper.get_table_id(match_soup)\n",
    "\n",
    "        # Scrapear y procesar los datos de los partidos\n",
    "        matches = scraper.scrape_league_data(match_url, liga)\n",
    "        for match in matches:\n",
    "            match['liga'] = liga  # Asegurar que cada registro tenga el nombre de la liga\n",
    "        processor.process_data(matches)  # Procesar los datos de los partidos\n",
    "\n",
    "        # Obtener el ID de la tabla de estadísticas de equipos\n",
    "        stats_response = scraper.make_request(stats_url)\n",
    "        stats_soup = BeautifulSoup(stats_response.text, 'html.parser')\n",
    "        table_id_stats = scraper.get_table_id(stats_soup)\n",
    "\n",
    "        # Scrapear y procesar las estadísticas de los equipos\n",
    "        if table_id_stats:  # Solo intentar si 'table_id_stats' está presente\n",
    "            team_stats = scraper.scrape_team_stats(stats_url, liga, table_id_stats)\n",
    "            for team_stat in team_stats:\n",
    "                team_stat['liga'] = liga  # Asegurar que cada registro tenga el nombre de la liga\n",
    "            processor.process_data(team_stats)  # Procesar las estadísticas de los equipos\n",
    "        else:\n",
    "            print(f\"No se encontró el ID de la tabla de estadísticas del equipo en la página: {stats_url}\")\n",
    "\n",
    "        # Mensaje de progreso\n",
    "        print(f\"Datos scrapeados y procesados para la liga {liga}\")\n",
    "        \n",
    "    except KeyError as e:\n",
    "        print(f\"Falta una clave requerida en la información de la liga {liga}: {e}\")\n",
    "\n",
    "\n",
    "# Guardar todos los datos procesados en un único archivo Excel\n",
    "processor.save_to_excel('datos_ligas.xlsx')\n",
    "\n",
    "# Guardar todos los datos procesados en un único archivo JSON\n",
    "processor.save_to_json('datos_ligas.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "\n",
    "    def process_data(self, data):\n",
    "        \"\"\"\n",
    "        Convierte los datos (lista de diccionarios) en un DataFrame y los almacena.\n",
    "        \"\"\"\n",
    "        if isinstance(data, list) and all(isinstance(item, dict) for item in data):\n",
    "            df = pd.DataFrame(data)\n",
    "            self.data.append(df)\n",
    "        else:\n",
    "            print(\"Los datos proporcionados no son una lista de diccionarios.\")\n",
    "\n",
    "    def save_to_json(self, filename):\n",
    "        \"\"\"\n",
    "        Guarda los DataFrames en un archivo JSON, convirtiendo cada DataFrame en una lista de diccionarios.\n",
    "        \"\"\"\n",
    "        data_to_dump = [df.to_dict(orient='records') for df in self.data]\n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(data_to_dump, file, indent=4)\n",
    "        print(f\"Datos guardados en el archivo JSON: {filename}\")\n",
    "\n",
    "    def save_to_excel(self, filename):\n",
    "        \"\"\"\n",
    "        Guarda cada DataFrame en una hoja separada de un archivo Excel.\n",
    "        \"\"\"\n",
    "        with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
    "            for i, df in enumerate(self.data, 1):\n",
    "                sheet_name = f'Sheet_{i}' if 'liga' not in df.columns else df['liga'].iloc[0]\n",
    "                df.to_excel(writer, sheet_name=sheet_name[:31], index=False)  # Excel limita los nombres de las hojas a 31 caracteres\n",
    "        print(f\"Datos guardados en el archivo Excel: {filename}\")\n",
    "\n",
    "\n",
    "    def save_team_data(self, data, liga):\n",
    "        base_dir = f\"data/{liga}\"\n",
    "        os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "        for item in data:\n",
    "            team = item.get('Team') or item.get('equipo')  # Asume que 'Team' o 'equipo' están en tus datos\n",
    "            if not team:\n",
    "                continue\n",
    "\n",
    "            team_dir = os.path.join(base_dir, team)\n",
    "            os.makedirs(team_dir, exist_ok=True)\n",
    "\n",
    "            # Decide el tipo de datos (partidos, posiciones, jugadores) basado en la estructura de 'item'\n",
    "            if 'Match Report' in item:\n",
    "                tipo = 'tabla-partidos'\n",
    "            elif 'Pts' in item:\n",
    "                tipo = 'tabla-posiciones'\n",
    "            else:\n",
    "                tipo = 'tabla-jugadores'  # O cualquier otro tipo que tengas\n",
    "\n",
    "            file_path = os.path.join(team_dir, f\"{tipo}.xlsx\")\n",
    "            df = pd.DataFrame([item])\n",
    "            df.to_excel(file_path, index=False)\n",
    "\n",
    "            print(f\"Datos guardados para {team} en {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'equipo'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m matches \u001b[38;5;241m=\u001b[39m scraper\u001b[38;5;241m.\u001b[39mscrape_league_data(league[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatch_url\u001b[39m\u001b[38;5;124m\"\u001b[39m], liga)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m match \u001b[38;5;129;01min\u001b[39;00m matches:\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# Asegúrate de que cada 'match' incluya un campo 'equipo'\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m     guardar_datos_equipo([match], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtabla-partidos\u001b[39m\u001b[38;5;124m\"\u001b[39m, liga, \u001b[43mmatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mequipo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Scrapear y procesar las estadísticas de los equipos\u001b[39;00m\n\u001b[0;32m     35\u001b[0m team_stats \u001b[38;5;241m=\u001b[39m scraper\u001b[38;5;241m.\u001b[39mscrape_team_stats(league[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstats_url\u001b[39m\u001b[38;5;124m\"\u001b[39m], liga, league\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid_stats\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mKeyError\u001b[0m: 'equipo'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def guardar_datos_equipo(datos, tipo_tabla, liga, equipo):\n",
    "    # Normaliza los nombres para el sistema de archivos\n",
    "    liga_norm = liga.replace(\" \", \"-\").lower()\n",
    "    equipo_norm = equipo.replace(\" \", \"-\").lower()\n",
    "    \n",
    "    # Crea el directorio si no existe\n",
    "    dir_path = f'data/{liga_norm}/{tipo_tabla}'\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    \n",
    "    # Ruta del archivo\n",
    "    file_path = f'{dir_path}/{equipo_norm}.csv'\n",
    "    \n",
    "    # Convierte los datos en un DataFrame y guárdalos en un archivo CSV\n",
    "    pd.DataFrame(datos).to_csv(file_path, index=False)\n",
    "    print(f'Datos guardados en {file_path}')\n",
    "\n",
    "\n",
    "# Instanciar el scraper\n",
    "scraper = FootballScraper()\n",
    "\n",
    "# Iterar sobre las URLs y realizar el scraping para cada una\n",
    "for league in urls:\n",
    "    liga = league[\"liga\"]  # Nombre de la liga\n",
    "\n",
    "    # Scrapear y procesar los datos de los partidos\n",
    "    matches = scraper.scrape_league_data(league[\"match_url\"], liga)\n",
    "    for match in matches:\n",
    "        # Asegúrate de que cada 'match' incluya un campo 'equipo'\n",
    "        guardar_datos_equipo([match], \"tabla-partidos\", liga, match['equipo'])\n",
    "\n",
    "    # Scrapear y procesar las estadísticas de los equipos\n",
    "    team_stats = scraper.scrape_team_stats(league[\"stats_url\"], liga, league.get(\"id_stats\", \"\"))\n",
    "    for stat in team_stats:\n",
    "        # Asegúrate de que cada 'stat' incluya un campo 'equipo'\n",
    "        guardar_datos_equipo([stat], \"tabla-posiciones\", liga, stat['equipo'])\n",
    "\n",
    "    print(f\"Datos scrapeados y procesados para la liga {liga}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3121sii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
